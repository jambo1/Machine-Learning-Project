{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy as np, textblob, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comment object\n",
    "class Comment:\n",
    "    def __init__(self, label, text):\n",
    "        self.label = label\n",
    "        self.text = text\n",
    "        self.split_text = self.text.split()\n",
    "  \n",
    "    def __iter__(self):\n",
    "        for i in range [0:len(self.split_text)-1] :\n",
    "            return(self.split_text[i])\n",
    "    \n",
    "    def __next__(self):\n",
    "        if i < len(self.split_text)-1:\n",
    "            return self.split_text[i]\n",
    "        else:\n",
    "            raise StopIteration  # Done iterating.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and create comment objects with the label and comment\n",
    "\n",
    "import csv\n",
    "\n",
    "raw_comments =[]\n",
    "\n",
    "with open(\"Project work/Shortened training set.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        raw_comments.append(Comment(row['label'],row['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I think a significant amount would be against spending their tax dollars on other people.\n"
     ]
    }
   ],
   "source": [
    "#Check it worked\n",
    "print(raw_comments[10].label,raw_comments[10].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopwords from the split_text, but keep them in the original text. <br>\n",
    "Add all comments with more than 3 words (after removing stopwords) to a new list comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments =[]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for comment in raw_comments:\n",
    "    for word in comment.split_text:\n",
    "        if word in stopwords.words('english'):\n",
    "            comment.split_text.remove(word)\n",
    "    if len(comment.split_text) >3:\n",
    "        comments.append(comment)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I figure the alt-Right would have a field day. ['I', 'figure', 'alt-Right', 'would', 'a', 'field', 'day.']\n"
     ]
    }
   ],
   "source": [
    "print(comments[100].text, comments[100].split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3703\n"
     ]
    }
   ],
   "source": [
    "print(len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jamie\\PycharmProjects\\DevProject\\venv\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'classify'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0411ccc4cf51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msentim_analy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msentim_analy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\DevProject\\venv\\lib\\site-packages\\nltk\\sentiment\\sentiment_analyzer.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, instance)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \"\"\"\n\u001b[0;32m    116\u001b[0m         \u001b[0minstance_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance_feats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_feat_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'classify'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "sentim_analy = SentimentAnalyzer()\n",
    "\n",
    "sentim_analy.classify(comments[0].split_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create TextBlobs of all of the textual comments. This is useful for extracting a lot of information on the text, such as sentiment, nouns, verbs,word and noun frequencies, inflection, spelling corrections, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_blobs = []\n",
    "for comment in comments:\n",
    "    #Leave out the analyzer to default to the pattern analyser\n",
    "    comment_blobs.append(TextBlob(comment.text, analyzer=NaiveBayesAnalyzer() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[](/yeah) I have one answer, and one answer only: YEEEEEAAAAH!\n"
     ]
    }
   ],
   "source": [
    "print(comment_blobs[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech lists, giving each word in the sentence and what it is (i.e. noun, verb, adverb, etc. <br>\n",
    "Sentiment lists, giving the polarity (between -1 and 1) and subjectivity (0.0 being very objective 1.0 being very subjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_pos = []\n",
    "comment_sentiment =[]\n",
    "comment_words = []\n",
    "comment_sentences = []\n",
    "comment_noun_phrases = []\n",
    "\n",
    "for blob in comment_blobs:\n",
    "    comment_pos.append(blob.tags)\n",
    "    comment_sentiment.append(blob.sentiment)\n",
    "    comment_words.append(blob.words)\n",
    "    comment_sentences.append(blob.sentences)\n",
    "    comment_noun_phrases.append(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because it's what really bothers him... and it's a sign of weakness. Sentiment(polarity=0.2, subjectivity=0.2) ['because', 'it', \"'s\", 'what', 'really', 'bothers', 'him', 'and', 'it', \"'s\", 'a', 'sign', 'of', 'weakness'] [Sentence(\"because it's what really bothers him... and it's a sign of weakness.\")] []\n"
     ]
    }
   ],
   "source": [
    "ind = 11\n",
    "print(comment_blobs[ind], comment_sentiment[ind], comment_words[ind], comment_sentences[ind], comment_noun_phrases[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_nouns = []\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "extractor = ConllExtractor\n",
    "thisblob = TextBlob(comments[ind].text, np_extractor=extractor)\n",
    "print(thisblob.noun_phrases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
