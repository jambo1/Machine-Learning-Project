{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy as np, textblob, string\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create comment objects to hold desired information.<br>\n",
    "Create TextBlobs of all of the textual comments. This is useful for extracting a lot of information on the text, such as sentiment, nouns, verbs,word and noun frequencies, inflection, spelling corrections, etc.<br>\n",
    "Part of speech lists, giving each word in the sentence and what it is (i.e. noun, verb, adverb, etc. <br>\n",
    "Sentiment analysis is performed using VADER which gives positive, negative, neutral and compound scores. The compound score represents the aggregate of the other scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comment object\n",
    "class Comment:\n",
    "    def __init__(self, label, text):\n",
    "        self.label = label\n",
    "        self.text = text\n",
    "        self.split_text = text.split()\n",
    "        self.blob = TextBlob(self.text)\n",
    "        self.pos = self.blob.tags\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        self.sentiment = self.analyzer.polarity_scores(self.text)\n",
    "        self.words = self.blob.words\n",
    "        self.sentences = self.blob.sentences\n",
    "        self.noun_phrases = self.blob.noun_phrases\n",
    "        \n",
    "        length = int(len(self.words)/2)\n",
    "        first =\"\"\n",
    "        second = \"\"\n",
    "        for x in range(0,length):\n",
    "            first += self.words[x]+\" \"\n",
    "        for x in range(length, len(self.words)-1):\n",
    "            second += self.words[x]+\" \"\n",
    "        self.firstSent = self.analyzer.polarity_scores(first)\n",
    "        self.secondSent = self.analyzer.polarity_scores(second)\n",
    "        \n",
    "  \n",
    "    def __iter__(self):\n",
    "        for i in range [0:len(self.split_text)-1] :\n",
    "            return(self.split_text[i])\n",
    "    \n",
    "    def __next__(self):\n",
    "        if i < len(self.split_text)-1:\n",
    "            return self.split_text[i]\n",
    "        else:\n",
    "            raise StopIteration  # Done iterating.\n",
    "        \n",
    "    def getInfo(self):\n",
    "        print(\"Label: \" ,self.label,\" Text: \", self.text, \"Sentiment: \",self.sentiment,\" Noun-phrases\"\n",
    "              , self.noun_phrases,\"Parts of speech: \", self.pos)\n",
    "        \n",
    "    def getSarc(self):\n",
    "        length = int(len(self.words)/2)\n",
    "        first =\"\"\n",
    "        second = \"\"\n",
    "        for x in range(0,length):\n",
    "            first += self.words[x]+\" \"\n",
    "        for x in range(length, len(self.words)-1):\n",
    "            second += self.words[x]+\" \"\n",
    "        firstSent = self.analyzer.polarity_scores(first)\n",
    "        secondSent = self.analyzer.polarity_scores(second)\n",
    "        return firstSent, secondSent\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  1  Text:  Man sometimes it is so great when you get to the shop and realise you have no money Sentiment:  {'neg': 0.094, 'neu': 0.683, 'pos': 0.224, 'compound': 0.6176}  Noun-phrases [] Parts of speech:  [('Man', 'NN'), ('sometimes', 'VBZ'), ('it', 'PRP'), ('is', 'VBZ'), ('so', 'RB'), ('great', 'JJ'), ('when', 'WRB'), ('you', 'PRP'), ('get', 'VBP'), ('to', 'TO'), ('the', 'DT'), ('shop', 'NN'), ('and', 'CC'), ('realise', 'NN'), ('you', 'PRP'), ('have', 'VBP'), ('no', 'DT'), ('money', 'NN')]\nNone\n({'neg': 0.0, 'neu': 0.604, 'pos': 0.396, 'compound': 0.7384}, {'neg': 0.239, 'neu': 0.761, 'pos': 0.0, 'compound': -0.296})\n"
     ]
    }
   ],
   "source": [
    "#Check it worked\n",
    "comment = Comment(1, \"Man sometimes it is so great when you get to the shop and realise you have no money\")\n",
    "print(comment.getInfo())\n",
    "print(comment.getSarc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and create comment objects with the label and comment\n",
    "\n",
    "import csv\n",
    "\n",
    "raw_comments =[]\n",
    "\n",
    "with open(\"DevProject/Project work/Blind test.csv\", encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if len(row['comment'].split())>3: \n",
    "            raw_comments.append(Comment(row['label'],row['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0  Text:  Wow... That looks nice. Sentiment:  {'neg': 0.0, 'neu': 0.517, 'pos': 0.483, 'compound': 0.4215}  Noun-phrases ['wow'] Parts of speech:  [('Wow', 'NN'), ('That', 'DT'), ('looks', 'VBZ'), ('nice', 'JJ')]\nNone\n({'neg': 0.0, 'neu': 0.208, 'pos': 0.792, 'compound': 0.5859}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n4264\n"
     ]
    }
   ],
   "source": [
    "#Check it worked\n",
    "print(raw_comments[27].getInfo())\n",
    "print(raw_comments[27].getSarc())\n",
    "print(len(raw_comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords has little to no effect on the sentiment scores, however it may influence the algorithmg based on the content for detecting sarcasm through common words etc. <br>\n",
    "All comments of less than 3 words are also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarc_comments =[]\n",
    "\n",
    "for comment in raw_comments:\n",
    "    if comment.label == \"1\":\n",
    "        sarc_comments.append(comment)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  1  Text:  but it says \"fingerprint\", if I take a toeprint it would be a bureaucratic anomaly! Sentiment:  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  Noun-phrases [] Parts of speech:  [('but', 'CC'), ('it', 'PRP'), ('says', 'VBZ'), ('fingerprint', 'NN'), ('if', 'IN'), ('I', 'PRP'), ('take', 'VBP'), ('a', 'DT'), ('toeprint', 'NN'), ('it', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('a', 'DT'), ('bureaucratic', 'JJ'), ('anomaly', 'NN')]\nNone\n"
     ]
    }
   ],
   "source": [
    "print(sarc_comments[107].getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sarc comments  2428 . Mixed pos and neg  120 neg  0  pos  1180  neu  2428\nPercentages: Mixed pos and neg  4.942339373970346 neg  0.0  pos  48.59967051070841  neu  100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nNumber of comments  4264 . Mixed pos and neg  187 neg  0  pos  2037  neu  4262\nPercentages: Mixed pos and neg  4.385553470919325 neg  0.0  pos  47.77204502814259  neu  99.953095684803\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "negCount=0\n",
    "posCount=0\n",
    "neuCount=0\n",
    "total = len(sarc_comments)\n",
    "\n",
    "for comment in sarc_comments:\n",
    "    if comment.firstSent['neg'] < comment.firstSent['pos']:\n",
    "        if comment.secondSent['pos'] <comment.secondSent['neg']:\n",
    "            count+=1\n",
    "     \n",
    "    if comment.sentiment['neu'] >0.0:\n",
    "        neuCount+=1\n",
    "     \n",
    "    if comment.sentiment['pos'] >0.0:\n",
    "        posCount+=1\n",
    "print(\"Number of sarc comments \", total,\". Mixed pos and neg \",count, \"neg \",negCount, \" pos \", posCount, \" neu \", neuCount)\n",
    "print(\"Percentages: Mixed pos and neg \",(count/total)*100, \"neg \",(negCount/total)*100, \n",
    "      \" pos \", (posCount/total)*100, \" neu \", (neuCount/total)*100)\n",
    "count =0\n",
    "negCount=0\n",
    "posCount=0\n",
    "neuCount=0\n",
    "total = len(raw_comments)\n",
    "\n",
    "for comment in raw_comments:\n",
    "    if comment.firstSent['neg'] < comment.firstSent['pos']:\n",
    "        if comment.secondSent['pos'] <comment.secondSent['neg']:\n",
    "            count+=1\n",
    "     \n",
    "    if comment.sentiment['neu'] >0.0:\n",
    "        neuCount+=1\n",
    "     \n",
    "    if comment.sentiment['pos'] >0.0:\n",
    "        posCount+=1\n",
    "print()\n",
    "print(\"Number of comments \", len(raw_comments),\". Mixed pos and neg \",count, \"neg \",negCount, \" pos \", posCount, \" neu \", neuCount)\n",
    "print(\"Percentages: Mixed pos and neg \",(count/total)*100, \"neg \",(negCount/total)*100, \n",
    "      \" pos \", (posCount/total)*100, \" neu \", (neuCount/total)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears no overall correlation in the sample between sentiment and sarcasm from the sample except that all contain neutral sentiment, however this is true for the entire sample so does not distinctly apply to sarcastic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np_extractor must be an instance of BaseNPExtractor",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a11d59d22952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mextractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConllExtractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mthisblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_comments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_extractor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextractor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthisblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_phrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstripped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowerstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         _initialize_models(self, tokenizer, pos_tagger, np_extractor, analyzer,\n\u001b[1;32m--> 372\u001b[1;33m                            parser, classifier)\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m_initialize_models\u001b[1;34m(obj, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier)\u001b[0m\n\u001b[0;32m    320\u001b[0m     obj.np_extractor = _validated_param(np_extractor, \"np_extractor\",\n\u001b[0;32m    321\u001b[0m                                         \u001b[0mbase_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBaseNPExtractor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                                         default=BaseBlob.np_extractor)\n\u001b[0m\u001b[0;32m    323\u001b[0m     obj.pos_tagger = _validated_param(pos_tagger, \"pos_tagger\",\n\u001b[0;32m    324\u001b[0m                                         BaseTagger, BaseBlob.pos_tagger)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m_validated_param\u001b[1;34m(obj, name, base_class, default, base_class_name)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         raise ValueError('{name} must be an instance of {cls}'\n\u001b[1;32m--> 308\u001b[1;33m                          .format(name=name, cls=base_class_name))\n\u001b[0m\u001b[0;32m    309\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np_extractor must be an instance of BaseNPExtractor"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "alt_nouns = []\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "extractor = ConllExtractor\n",
    "thisblob = TextBlob(raw_comments[100].text, np_extractor=extractor)\n",
    "print(thisblob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letters: 52.01383677298311  Symbols:  12.974437148217635\nLetters: 50.55215182307232  Symbols:  12.57531380753138\n"
     ]
    }
   ],
   "source": [
    "ratio =[0,0]\n",
    "count =0\n",
    "for comment in raw_comments:\n",
    "    count+=1\n",
    "    thisrat = [0,0]\n",
    "    for i in range (0, len(comment.text)-1):\n",
    "        char = comment.text[i]\n",
    "        if char.isalpha():\n",
    "            ratio[0]+=1\n",
    "        else:\n",
    "            ratio[1]+=1\n",
    "    \n",
    "print (\"Letters:\",ratio[0]/count, \" Symbols: \",ratio[1]/count)\n",
    "\n",
    "\n",
    "for comment in sarc_comments:\n",
    "    count+=1\n",
    "    thisrat = [0,0]\n",
    "    for i in range (0, len(comment.text)-1):\n",
    "        char = comment.text[i]\n",
    "        if char.isalpha():\n",
    "            ratio[0]+=1\n",
    "        else:\n",
    "            ratio[1]+=1\n",
    "    \n",
    "print (\"Letters:\",ratio[0]/count, \" Symbols: \",ratio[1]/count)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be no difference in the use of symbols in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in raw_comments:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
